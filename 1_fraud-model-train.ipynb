{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "data_file='data/creditcard.csv'\n",
    "data=pd.read_csv(data_file,header=0)\n",
    "print(data.dtypes.head())\n",
    "print('Number of records:',len(data))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "feature_names=data.columns.values[:-1]\n",
    "train_test_set = data[data.Class==0][feature_names]\n",
    "train_set, test_set = train_test_split(train_test_set, test_size=0.2, random_state=42)\n",
    "print('Whole data set:',len(data))\n",
    "print('Train and test set:',len(train_test_set))\n",
    "train_set.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing with StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "# import matplotlib.pyplot as plt\n",
    "# %matplotlib inline\n",
    "# scaler1=MinMaxScaler().fit(train_set[['Time']])\n",
    "# train_set['Time']=scaler1.transform(train_set[['Time']])\n",
    "# test_set['Time']=scaler1.transform(test_set[['Time']])\n",
    "# scaler2=StandardScaler().fit(train_set[['Amount']])\n",
    "# train_set['Amount']=scaler2.transform(train_set[['Amount']])\n",
    "# test_set['Amount']=scaler2.transform(test_set[['Amount']])\n",
    "\n",
    "# train_set.hist(column=['Time','Amount','V1','V2'],bins=100)\n",
    "# train_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_set=train_set.to_numpy()\n",
    "# test_set=test.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing using MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "scaler=MinMaxScaler().fit(train_set)\n",
    "train_set=scaler.transform(train_set)\n",
    "test_set=scaler.transform(test_set)\n",
    "\n",
    "pd.Series(train_set[:,0]).hist(bins=100)\n",
    "pd.Series(train_set[:,1]).hist(bins=100)\n",
    "pd.Series(train_set[:,2]).hist(bins=100)\n",
    "pd.Series(train_set[:,-1]).hist(bins=100)\n",
    "train_set[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class autoencoder(nn.Module):\n",
    "    def __init__(self,num_input):\n",
    "        super(autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(num_input, 15),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(15, 7))\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(7, 15),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(15, num_input),\n",
    "            nn.Tanh())\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 300\n",
    "batch_size = 256\n",
    "lr = 0.01\n",
    "\n",
    "inputs = torch.tensor(train_set, dtype=torch.float32)\n",
    "dataset = TensorDataset(inputs)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, num_workers=4, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = autoencoder(inputs.shape[1])\n",
    "model = model.cuda() if torch.cuda.is_available() else model\n",
    "criterion = nn.MSELoss(reduction='sum')\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)\n",
    "\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        if torch.cuda.is_available():\n",
    "            tests_=torch.tensor(test_set, dtype=torch.float32).cuda()\n",
    "        else:\n",
    "            tests_=torch.tensor(test_set, dtype=torch.float32)\n",
    "        outputs = model(tests_)\n",
    "        loss=criterion(outputs,tests_)\n",
    "    #print(tests_,outputs)\n",
    "    return loss.item()/(tests_.shape[0]*tests_.shape[1])\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    loss_sum=0.0; num=0\n",
    "    for inputs, in dataloader:\n",
    "        if torch.cuda.is_available():\n",
    "            inputs=inputs.cuda()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, inputs)\n",
    "\n",
    "        loss_sum+=loss.item()\n",
    "        num+=(inputs.shape[0]*inputs.shape[1])\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    if (epoch+1)%5 == 0:\n",
    "        print('{} epoch [{}/{}], loss:{:.6f}, test_set_loss:{:.6f}'\n",
    "                .format(datetime.now(), epoch + 1, num_epochs, loss_sum/num, test()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.eval()\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     test_set2 = data[data.Class==1][feature_names]\n",
    "#     test_set2['Time']=scaler1.transform(test_set2[['Time']])\n",
    "#     test_set2['Amount']=scaler2.transform(test_set2[['Amount']])\n",
    "#     inputs2=torch.tensor(test_set2.to_numpy(), dtype=torch.float32)\n",
    "#     outputs2=model(inputs2)\n",
    "#     loss2=torch.sum((inputs2-outputs2)**2,dim=1).sqrt().log()\n",
    "\n",
    "#     test_set1=test_set.sample(n=len(loss2),random_state=42)\n",
    "#     inputs1=torch.tensor(test_set1.to_numpy(), dtype=torch.float32)\n",
    "#     outputs1=model(inputs1)\n",
    "#     loss1=torch.sum((inputs1-outputs1)**2,dim=1).sqrt().log()\n",
    "\n",
    "#     pd.Series(loss1.numpy()).hist(bins=100)\n",
    "#     pd.Series(loss2.numpy()).hist(bins=100)\n",
    "#     split_point=(loss1.max()+loss2.min())/2\n",
    "#     print('Split point:',split_point)\n",
    "#     print((loss1<split_point).sum().item()/float(len(loss1)))\n",
    "#     print((loss2>split_point).sum().item()/float(len(loss2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    test_set2 = data[data.Class==1][feature_names]\n",
    "    test_set2=scaler.transform(test_set2)\n",
    "    inputs2=torch.tensor(test_set2, dtype=torch.float32)\n",
    "    outputs2=model(inputs2)\n",
    "    loss2=torch.sum((inputs2-outputs2)**2,dim=1).sqrt().log()\n",
    "\n",
    "    test_set1=test_set[np.random.choice(len(test_set),size=len(loss2),replace=False)]\n",
    "    inputs1=torch.tensor(test_set1, dtype=torch.float32)\n",
    "    outputs1=model(inputs1)\n",
    "    loss1=torch.sum((inputs1-outputs1)**2,dim=1).sqrt().log()\n",
    "\n",
    "    pd.Series(loss1.numpy()).hist(bins=100)\n",
    "    pd.Series(loss2.numpy()).hist(bins=100)\n",
    "    split_point=(loss1.max()+loss2.min())/2\n",
    "    print('Split point:',split_point)\n",
    "    print((loss1<split_point).sum().item()/float(len(loss1)))\n",
    "    print((loss2>split_point).sum().item()/float(len(loss2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), '/Users/john/projects/cloudera-ml/anomaly-detection/creditcard-fraud-2.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3.6.4 64-bit ('miniconda3-latest': pyenv)",
   "language": "python",
   "name": "python36464bitminiconda3latestpyenvfaa952c22d91438a9b3b5ae49720d629"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
