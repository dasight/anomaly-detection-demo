{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Credit Card Fraud Model Training & Evaluation\n",
    "## Prerequisite\n",
    "\n",
    "Here, we present the steps of building and training a credit card fraud detection model. We use the autoencoder approach and implement with PyTorch. First of all, import necessary Python modules, especially Numpy, Pandas and PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the Kaggle [Credit Card Fraud Detection](https://www.kaggle.com/mlg-ulb/creditcardfraud) dataset here. The datasets contains transactions made by credit cards in September 2013 by European cardholders, and presents transactions that occurred in two days, where we have 492 frauds out of 284,807 transactions. The dataset is highly unbalanced, the positive class (frauds) account for 0.172% of all transactions.\n",
    "\n",
    "The dataset is in CSV format, with the first row as the header. We first load it as a Pandas DataFrame, and verify its positive and negative samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records: 284807\n",
      "Positive samples: 284315\n",
      "Negative samples: 492\n"
     ]
    }
   ],
   "source": [
    "data_file='data/creditcard.csv'\n",
    "data=pd.read_csv(data_file,header=0)\n",
    "print('Number of records:',len(data))\n",
    "print('Positive samples:',len(data[data.Class==0]))\n",
    "print('Negative samples:',len(data[data.Class==1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset looks like the following. It contains only numerical input variables which are the result of a PCA transformation. Features V1, V2, … V28 are the principal components obtained with PCA, the only features which have not been transformed with PCA are 'Time' and 'Amount'. Feature 'Time' contains the seconds elapsed between each transaction and the first transaction in the dataset. The feature 'Amount' is the transaction Amount, and the feature 'Class' is the response variable and it takes value 1 in case of fraud and 0 otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
       "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
       "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
       "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
       "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
       "\n",
       "         V8        V9  ...       V21       V22       V23       V24       V25  \\\n",
       "0  0.098698  0.363787  ... -0.018307  0.277838 -0.110474  0.066928  0.128539   \n",
       "1  0.085102 -0.255425  ... -0.225775 -0.638672  0.101288 -0.339846  0.167170   \n",
       "2  0.247676 -1.514654  ...  0.247998  0.771679  0.909412 -0.689281 -0.327642   \n",
       "3  0.377436 -1.387024  ... -0.108300  0.005274 -0.190321 -1.175575  0.647376   \n",
       "4 -0.270533  0.817739  ... -0.009431  0.798278 -0.137458  0.141267 -0.206010   \n",
       "\n",
       "        V26       V27       V28  Amount  Class  \n",
       "0 -0.189115  0.133558 -0.021053  149.62      0  \n",
       "1  0.125895 -0.008983  0.014724    2.69      0  \n",
       "2 -0.139097 -0.055353 -0.059752  378.66      0  \n",
       "3 -0.221929  0.062723  0.061458  123.50      0  \n",
       "4  0.502292  0.219422  0.215153   69.99      0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing\n",
    "### Choosing positive samples\n",
    "\n",
    "The autoencoder for fraud detection uses only the positive (non-fraud) samples to train the model, thus only chooses records with Class==0. Then we split the chosen records as train set and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train and test set: 284315\n"
     ]
    }
   ],
   "source": [
    "feature_names=data.columns.values[:-1]\n",
    "train_test_set = data[data.Class==0][feature_names]\n",
    "print('Train and test set:',len(train_test_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting into train set and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_set, test_set = train_test_split(train_test_set, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing using MinMaxScaler\n",
    "\n",
    "Use Sklearn MinMaxScaler to transform the data value ranges to 0..1. We show the value of the first two rows after transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[8.42327013e-01, 9.47367642e-01, 7.97217483e-01, 7.76227227e-01,\n",
       "        2.41740662e-01, 5.43571003e-01, 5.15661441e-01, 4.19171267e-01,\n",
       "        8.02040109e-01, 3.17799008e-01, 3.49988397e-01, 2.35464057e-01,\n",
       "        6.15377573e-01, 4.10107015e-01, 5.99855337e-01, 4.48405051e-01,\n",
       "        6.02247154e-01, 6.71423155e-01, 6.12703253e-01, 5.13449227e-01,\n",
       "        4.11165178e-01, 6.10928994e-01, 5.39959714e-01, 6.67924552e-01,\n",
       "        5.05950311e-01, 5.33984885e-01, 5.03033612e-01, 6.46853527e-01,\n",
       "        2.57966091e-01, 5.09856012e-03],\n",
       "       [2.44658194e-01, 9.79105547e-01, 7.97062325e-01, 7.83882282e-01,\n",
       "        3.03888114e-01, 5.48372609e-01, 5.15350500e-01, 4.21609233e-01,\n",
       "        7.95745386e-01, 2.79267731e-01, 3.87358807e-01, 3.75257082e-01,\n",
       "        6.96864586e-01, 5.41487510e-01, 6.50882451e-01, 3.64243237e-01,\n",
       "        5.45108394e-01, 6.30551391e-01, 5.31712561e-01, 5.85659779e-01,\n",
       "        4.13125001e-01, 6.06201154e-01, 5.15266633e-01, 6.62486089e-01,\n",
       "        4.14393519e-01, 6.25814080e-01, 3.86877333e-01, 6.50121975e-01,\n",
       "        2.57044176e-01, 5.08736791e-05]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler=MinMaxScaler().fit(train_set)\n",
    "train_set=scaler.transform(train_set)\n",
    "test_set=scaler.transform(test_set)\n",
    "train_set[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moreover, we can also check the distribution of each feature. The following diagram shows the distribution of the first three and the last features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x118c49f98>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "pd.Series(train_set[:,0]).hist(bins=100)\n",
    "pd.Series(train_set[:,1]).hist(bins=100)\n",
    "pd.Series(train_set[:,2]).hist(bins=100)\n",
    "pd.Series(train_set[:,-1]).hist(bins=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling and training\n",
    "### Model definition\n",
    "\n",
    "Define the autoencoder model with PyTorch. Here, we define 2 hidden layer for each of the encoder and decoder, with 15 and 7 cells respectively, and use ReLU as the activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class autoencoder(nn.Module):\n",
    "    def __init__(self,num_input):\n",
    "        super(autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(num_input, 15),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(15, 7))\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(7, 15),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(15, num_input),\n",
    "            nn.Tanh())\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "num_workers = 4\n",
    "\n",
    "inputs = torch.tensor(train_set, dtype=torch.float32)\n",
    "dataset = TensorDataset(inputs)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, num_workers=num_workers, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining loss function and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.01\n",
    "\n",
    "model = autoencoder(inputs.shape[1])\n",
    "model = model.cuda() if torch.cuda.is_available() else model\n",
    "criterion = nn.MSELoss(reduction='sum')\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining test function\n",
    "\n",
    "The test function is used for cross validation during the training. Here we choose to do cross validation for each 5 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        if torch.cuda.is_available():\n",
    "            tests_=torch.tensor(test_set, dtype=torch.float32).cuda()\n",
    "        else:\n",
    "            tests_=torch.tensor(test_set, dtype=torch.float32)\n",
    "        outputs = model(tests_)\n",
    "        loss=criterion(outputs,tests_)\n",
    "    #print(tests_,outputs)\n",
    "    return loss.item()/(tests_.shape[0]*tests_.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-05-10 18:32:05.161102 epoch [5/100], loss:0.001350, test_set_loss:0.001348\n",
      "2020-05-10 18:32:11.249185 epoch [10/100], loss:0.001302, test_set_loss:0.001304\n",
      "2020-05-10 18:32:17.820717 epoch [15/100], loss:0.001282, test_set_loss:0.001275\n",
      "2020-05-10 18:32:24.841254 epoch [20/100], loss:0.001267, test_set_loss:0.001259\n",
      "2020-05-10 18:32:31.619783 epoch [25/100], loss:0.001244, test_set_loss:0.001235\n",
      "2020-05-10 18:32:38.562468 epoch [30/100], loss:0.001235, test_set_loss:0.001236\n",
      "2020-05-10 18:32:44.932958 epoch [35/100], loss:0.001235, test_set_loss:0.001231\n",
      "2020-05-10 18:32:51.896279 epoch [40/100], loss:0.001227, test_set_loss:0.001231\n",
      "2020-05-10 18:32:59.101634 epoch [45/100], loss:0.001225, test_set_loss:0.001230\n",
      "2020-05-10 18:33:06.707046 epoch [50/100], loss:0.001225, test_set_loss:0.001233\n",
      "2020-05-10 18:33:14.098740 epoch [55/100], loss:0.001221, test_set_loss:0.001220\n",
      "2020-05-10 18:33:21.447746 epoch [60/100], loss:0.001223, test_set_loss:0.001231\n",
      "2020-05-10 18:33:28.704236 epoch [65/100], loss:0.001219, test_set_loss:0.001234\n",
      "2020-05-10 18:33:36.587522 epoch [70/100], loss:0.001216, test_set_loss:0.001247\n",
      "2020-05-10 18:33:42.968078 epoch [75/100], loss:0.001215, test_set_loss:0.001236\n",
      "2020-05-10 18:33:49.864205 epoch [80/100], loss:0.001280, test_set_loss:0.001672\n",
      "2020-05-10 18:33:57.177136 epoch [85/100], loss:0.001501, test_set_loss:0.001293\n",
      "2020-05-10 18:34:04.871800 epoch [90/100], loss:0.001276, test_set_loss:0.001272\n",
      "2020-05-10 18:34:11.953219 epoch [95/100], loss:0.001268, test_set_loss:0.001264\n",
      "2020-05-10 18:34:18.769562 epoch [100/100], loss:0.001268, test_set_loss:0.001265\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 100\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    loss_sum=0.0; num=0\n",
    "    for inputs, in dataloader:\n",
    "        if torch.cuda.is_available():\n",
    "            inputs=inputs.cuda()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, inputs)\n",
    "\n",
    "        loss_sum+=loss.item()\n",
    "        num+=(inputs.shape[0]*inputs.shape[1])\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    if (epoch+1)%5 == 0:\n",
    "        print('{} epoch [{}/{}], loss:{:.6f}, test_set_loss:{:.6f}'\n",
    "                .format(datetime.now(), epoch + 1, num_epochs, loss_sum/num, test()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model evaluation\n",
    "### Inference with trained model\n",
    "\n",
    "To inference with the trained model, we do the following:\n",
    "\n",
    "* Preprocess the records using the MinMaxScaler (the *scaler* variable) defined and trained in the *Data preprocessing using MinMaxScaler* section.\n",
    "* Use the preprocessed data as the input vectors of the model, and compute the output vectors by feed forward.\n",
    "* Calculate the root square of the input and output vectors, i.e. the generation losses.\n",
    "* For those whose generation losses are greater than a predefined threshold (as for how to calculate the value of the threshold, see the discussion below), we score them as *fraud*; otherwise, score as *normal*.\n",
    "\n",
    "The value of the threshold is actually calculated via the distribution of the generation losses. Thus let's first show the generation losses of the trained model for the *normal* and *fraud* cases.\n",
    "\n",
    "### Distribution of generation losses\n",
    "\n",
    "As the dataset is highly unbalanced, we take the whole positive (fraud) records while randomly choosing the equal number of negative (normal) records. The diagram below shows the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    test_set2 = data[data.Class==1][feature_names]\n",
    "    test_set2=scaler.transform(test_set2)\n",
    "    inputs2=torch.tensor(test_set2, dtype=torch.float32)\n",
    "    outputs2=model(inputs2)\n",
    "    loss2=torch.sum((inputs2-outputs2)**2,dim=1).sqrt().log()\n",
    "\n",
    "    test_set1=test_set[np.random.choice(len(test_set),size=len(loss2),replace=False)]\n",
    "    inputs1=torch.tensor(test_set1, dtype=torch.float32)\n",
    "    outputs1=model(inputs1)\n",
    "    loss1=torch.sum((inputs1-outputs1)**2,dim=1).sqrt().log()\n",
    "\n",
    "    pd.Series(loss1.numpy()).hist(bins=100)\n",
    "    pd.Series(loss2.numpy()).hist(bins=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculation of threshold (*split point*)\n",
    "\n",
    "If the distribution functions of the generation losses are the convex function, the threshold is very easy to calculation. It is the just the intersection of the positive-case and negative-case distribution functions. However, as we see from the diagram above, neither of the distribution function is strictly convex.\n",
    "\n",
    "Here, we define the threshold as the split point that maximize the average precise in both positive and negative cases. And use a *5-points* heuristic search approach to find the split point (see the **find_split_point** function).\n",
    "\n",
    "Initially, we define the search range as the minimum generation loss of the positive cases and the maximum of the negative cases. For each iteration, we pick up half of the search range using the heuristic function to evaluate the picked range contains the split point. When the search range is smaller than 0.01, the iterations stop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-2.1654911041259766 -> -0.46983790397644043\n",
      "-1.3176645040512085 -> -0.46983790397644043\n",
      "-1.3176645040512085 -> -0.8937512040138245\n",
      "-1.2116861790418625 -> -0.9997295290231705\n",
      "-1.2116861790418625 -> -1.1057078540325165\n",
      "-1.2116861790418625 -> -1.1586970165371895\n",
      "-1.185191597789526 -> -1.1586970165371895\n",
      "-1.1719443071633577 -> -1.1586970165371895\n",
      "-1.1653206618502736 -> -1.1586970165371895\n",
      "\n",
      "Split point: -1.1620088391937315\n"
     ]
    }
   ],
   "source": [
    "def precise_rate(split_point):\n",
    "    rate1=(loss1<split_point).sum().item()/float(len(loss1))\n",
    "    rate2=(loss2>split_point).sum().item()/float(len(loss2))\n",
    "    return (rate1+rate2)/2            \n",
    "\n",
    "def find_split_point(start,end,start_precise,end_precise):\n",
    "    print(start,'->',end)\n",
    "    delta=(end-start)/4.0\n",
    "    precise=[start_precise]\n",
    "    precise+=[precise_rate(start+i*delta) for i in range(1,4)]\n",
    "    precise+=[end_precise]\n",
    "\n",
    "    i = 0 if sum(precise[0:3])>sum(precise[1:4]) else 1\n",
    "    j = i if sum(precise[i:i+3])>sum(precise[2:5]) else 2\n",
    "\n",
    "    if end-start>0.01:\n",
    "        return find_split_point(start+j*delta,start+(j+2)*delta,precise[j],precise[j+2])\n",
    "    else:\n",
    "        return start+delta*np.argmax(precise)\n",
    "\n",
    "\n",
    "(start,end)=(loss1.max().item(),loss2.min().item())\n",
    "(start,end)=(start,end) if start<end else (end,start)\n",
    "split_point=find_split_point(start,end,precise_rate(start),precise_rate(end))\n",
    "print('\\nSplit point:',split_point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the outputs above, we can see how the search range shrinks with each iterations, and the final split point calculated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model precise rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precise rate for normal cases: 0.967479674796748\n",
      "Precise rate for fraud cases: 0.8719512195121951\n",
      "Overall precise: 0.9197154471544715\n"
     ]
    }
   ],
   "source": [
    "precise1=(loss1<split_point).sum().item()/float(len(loss1))\n",
    "precise2=(loss2>split_point).sum().item()/float(len(loss2))\n",
    "print('Precise rate for normal cases:',precise1)\n",
    "print('Precise rate for fraud cases:',precise2)\n",
    "print('Overall precise:',(precise1+precise2)/2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(model.state_dict(), 'creditcard-fraud-2.model')"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3.6.4 64-bit ('miniconda3-latest': pyenv)",
   "language": "python",
   "name": "python36464bitminiconda3latestpyenvfaa952c22d91438a9b3b5ae49720d629"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
